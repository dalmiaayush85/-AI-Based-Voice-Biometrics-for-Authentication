import os
import pandas as pd
import torchaudio
import torch
from torch.utils.data import Dataset, DataLoader
from speechbrain.pretrained import SpeakerRecognition, EncoderClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm
from torch import nn, optim

# Ensure reproducibility
torch.manual_seed(42)

# === STEP 1: Load and prepare your dataset ===
df = pd.read_csv("other.csv")
df = df.groupby('client_id').filter(lambda x: len(x) >= 2)

# Keep only required columns and construct full paths
df = df[['client_id', 'path', 'gender', 'age']]
df['full_path'] = df['path'].apply(lambda x: os.path.join("clips", x))

# === STEP 2: Dataset class ===
class SpeakerDataset(Dataset):
    def __init__(self, dataframe, sample_rate=16000, duration=3.0):
        self.df = dataframe.reset_index(drop=True)
        self.sample_rate = sample_rate
        self.fixed_length = int(sample_rate * duration)
        self.label_to_id = {label: idx for idx, label in enumerate(self.df['client_id'].unique())}
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        audio_path = row['full_path']
        waveform, sr = torchaudio.load(audio_path)
        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)
        waveform = waveform.squeeze(0)

        # Pad or truncate to fixed length
        if waveform.shape[0] < self.fixed_length:
            padding = self.fixed_length - waveform.shape[0]
            waveform = torch.nn.functional.pad(waveform, (0, padding))
        else:
            waveform = waveform[:self.fixed_length]

        label = self.label_to_id[row['client_id']]
        return waveform, label

# === STEP 3: Split data ===
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['client_id'], random_state=42)
train_dataset = SpeakerDataset(train_df)
test_dataset = SpeakerDataset(test_df)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

# === STEP 4: Load ECAPA-TDNN pretrained model ===
speaker_model = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir="models/spkrec"
)

# === STEP 5: Embedding extractor function ===
def get_embedding(waveform):
    with torch.no_grad():
        emb = speaker_model.encode_batch(waveform.unsqueeze(0)).squeeze(0).squeeze(0)
    return emb

# === STEP 6: Classifier on top of embeddings ===
class SpeakerClassifier(nn.Module):
    def __init__(self, emb_dim, num_classes):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(emb_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        return self.fc(x)

num_classes = len(train_dataset.label_to_id)
model = SpeakerClassifier(emb_dim=192, num_classes=num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# === STEP 7: Training loop ===
for epoch in range(5):  # Increase if needed
    model.train()
    total_loss = 0
    for waveforms, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
        embeddings = torch.stack([get_embedding(w) for w in waveforms])
        outputs = model(embeddings)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    print(f"âœ… Epoch {epoch+1} Loss: {total_loss:.4f}")

# === STEP 8: Evaluation ===
model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for waveforms, labels in test_loader:
        embeddings = torch.stack([get_embedding(w) for w in waveforms])
        outputs = model(embeddings)
        preds = torch.argmax(outputs, dim=1)
        y_true.extend(labels.numpy())
        y_pred.extend(preds.numpy())

print("\nðŸ“Š Classification Report:")
print(classification_report(y_true, y_pred))

# === STEP 9: Spoof detection model ===
spoof_model = EncoderClassifier.from_hparams(
    source="speechbrain/spoof-ecapa-voxceleb",
    savedir="models/aasist"
)

# === STEP 10: Spoof checker function ===
def check_spoof(audio_path):
    prediction = spoof_model.classify_file(audio_path)
    label = torch.argmax(prediction).item()
    return "spoof" if label == 1 else "real"

# === STEP 11: Run spoof check on one sample ===
sample_file = df.iloc[0]['full_path']
print(f"\nðŸ›¡ Spoof check for '{sample_file}': {check_spoof(sample_file)}")
